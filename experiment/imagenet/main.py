import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import time
import torch
import torch.nn.functional as F
from model import *

import os
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from util import Logger, Bar, AverageMeter, accuracy, load_dataset, warp_decay, split_params, init_config, bptt_model_setting
from spikingjelly.activation_based import functional
from model.layer import *
from model import ResNet_ANN

def train(train_ldr, optimizer, model, t_model, evaluator, args):
    model.train()
    t_model.eval()
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    end = time.time()

    bar = Bar('Processing', max=len(train_ldr))

    for idx, (ptns, labels) in enumerate(train_ldr):
        device = next(model.parameters()).device
        ptns, labels = ptns.to(device), labels.to(device)

        data_time.update(time.time() - end)

        optimizer.zero_grad()
        functional.reset_net(model)
        if model.step_mode == 's':
            out_spikes = []
            for t in range(args.T):
                out = model(ptns)
                out_spikes.append(out)
            output = torch.stack(out_spikes, dim=0)
            avg_fr = output.mean(dim=0)
        else:
            in_data, _ = torch.broadcast_tensors(ptns, torch.zeros((args.T,) + ptns.shape))
            in_data = in_data.reshape(-1, *in_data.shape[2:])
            output = model(in_data)
            output = output.reshape(args.T, -1, output.shape[-1])
            avg_fr = output.mean(dim=0)
        
        # SEAL (Selective Alignment) Loss
        hard_loss = cal_loss(output, labels, evaluator)
        
        # 1. Intra-SNN Distillation (Self/Time-step)
        loss_time = time_step_kd_loss(output, temperature=3, method=args.time_kd_method)

        # 2. Inter-Model Alignment (ANN Distillation)
        with torch.no_grad():
            t_avg_fr = t_model(ptns)
        
        loss_time2 = 0.0
        for i in range(args.T):
            s_aligned, t_aligned = align_teacher_student_logits(output[i], t_avg_fr, labels, method=args.align_method)
            loss_time2 += kd_loss(s_aligned, t_aligned.detach(), 3)
        loss_time2 = loss_time2 / args.T
        
        loss = hard_loss + loss_time2 * args.alpha + loss_time * args.beta

        loss.backward()
        optimizer.step()

        prec1, prec5 = accuracy(avg_fr.data, labels.data, topk=(1, 5))
        losses.update(loss.data.item(), ptns.size(0))
        top1.update(prec1.item(), ptns.size(0))
        top5.update(prec5.item(), ptns.size(0))

        batch_time.update(time.time() - end)
        end = time.time()

        bar.suffix = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(
            batch=idx + 1,
            size=len(train_ldr),
            data=data_time.avg,
            bt=batch_time.avg,
            total=bar.elapsed_td,
            eta=bar.eta_td,
            loss=losses.avg,
            top1=top1.avg,
            top5=top5.avg,
        )
        bar.next()
    bar.finish()

    return top1.avg, losses.avg


def test(val_ldr, model, evaluator, args):
    model.eval()
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()

    end = time.time()
    bar = Bar('Processing', max=len(val_ldr))

    with torch.no_grad():
        for idx, (ptns, labels_batch) in enumerate(val_ldr):
            ptns, labels_batch = ptns.to(next(model.parameters()).device), labels_batch.to(
                next(model.parameters()).device)

            functional.reset_net(model)
            if model.step_mode == 's':
                out_spikes = []
                for t in range(args.T):
                    out = model(ptns)
                    out_spikes.append(out)
                output = torch.stack(out_spikes, dim=0)  # [T, B, C]
                avg_fr = output.mean(dim=0)
            else:
                in_data, _ = torch.broadcast_tensors(ptns, torch.zeros((args.T,) + ptns.shape))
                in_data = in_data.reshape(-1, *in_data.shape[2:])
                output = model(in_data)
                output = output.reshape(args.T, -1, output.shape[-1])  # [T, B, C]
                avg_fr = output.mean(dim=0)
            
            loss = evaluator(avg_fr, labels_batch)

            prec1, prec5 = accuracy(avg_fr.data, labels_batch.data, topk=(1, 5))
            losses.update(loss.data.item(), ptns.size(0))
            top1.update(prec1.item(), ptns.size(0))
            top5.update(prec5.item(), ptns.size(0))
            
            batch_time.update(time.time() - end)
            end = time.time()
            bar.suffix = '({batch}/{size}) Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(
                batch=idx + 1,
                size=len(val_ldr),
                bt=batch_time.avg,
                total=bar.elapsed_td,
                eta=bar.eta_td,
                loss=losses.avg,
                top1=top1.avg,
                top5=top5.avg,
            )
            bar.next()
        bar.finish()
        
    return top1.avg, losses.avg

def seed_worker(worker_id):
    import random
    import numpy as np
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

def main():
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    dtype = torch.float
    log = Logger(args, args.log_path)
    log.info_args(args)
    writer = SummaryWriter(args.log_path)

    train_data, val_data, num_class = load_dataset(args.dataset, args.data_path, cutout=args.cutout,
                                                   auto_aug=args.auto_aug, time_step=args.T)

    g = torch.Generator()
    g.manual_seed(args.seed)

    train_ldr = DataLoader(dataset=train_data, batch_size=args.train_batch_size, shuffle=True,
                           pin_memory=True, num_workers=args.num_workers,
                           worker_init_fn=seed_worker, generator=g)
    val_ldr = DataLoader(dataset=val_data, batch_size=args.val_batch_size, shuffle=False,
                         pin_memory=True, num_workers=args.num_workers,
                         worker_init_fn=seed_worker, generator=g)

    kwargs_spikes = {'v_reset': args.v_reset, 'thresh': args.thresh, 'decay': warp_decay(args.decay),
                     'detach_reset': args.detach_reset}

    in_channels = 2 if 'dvs' in args.dataset.lower() else 3
    
    model = eval(args.stu_arch + f'(num_classes={num_class}, in_channel={in_channels}, **kwargs_spikes)')
    model.to(device, dtype)
    t_model = ResNet_ANN.__dict__[args.tea_arch](num_classes=num_class, in_channels=in_channels)
    t_model.to(device, dtype)

    bptt_model_setting(model, time_step=args.T, step_mode=args.step_mode)

    if torch.cuda.device_count() > 1:
        model = nn.DataParallel(model)
        t_model = nn.DataParallel(t_model)

    params = split_params(model)
    params = [
        {'params': params[1], 'weight_decay': args.wd},
        {'params': params[2], 'weight_decay': 0}
    ]

    if args.optim.lower() == 'sgdm':
        optimizer = optim.SGD(params, lr=args.lr, momentum=0.9)
    elif args.optim.lower() == 'adam':
        optimizer = optim.Adam(params, lr=args.lr, amsgrad=False)
    elif args.optim.lower() == 'adamw':
        optimizer = optim.AdamW(params, lr=args.lr)
    else:
        raise NotImplementedError(f"Optimizer {args.optim} not supported.")

    evaluator = torch.nn.CrossEntropyLoss()
    start_epoch = 0
    best_epoch = 0
    best_acc = 0.0

    if args.tea_path is not None:
        state = torch.load(args.tea_path, map_location=device, weights_only=True)
        t_model.load_state_dict(state['best_net'])
        log.info('Load checkpoint from {}'.format(args.tea_path))
        
    args.start_epoch = start_epoch
    if args.scheduler.lower() == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=args.num_epoch)
    else:
        raise NotImplementedError()
    
    train_start_time = time.time()
    for epoch in range(start_epoch, args.num_epoch):
        epoch_start_time = time.time()
        train_acc, train_loss = train(train_ldr, optimizer, model, t_model, evaluator, args=args)
        
        if args.scheduler != 'None':
            scheduler.step()
        
        val_acc, val_loss = test(val_ldr, model, evaluator, args=args)
        
        epoch_time = time.time() - epoch_start_time
        
        if val_acc > best_acc:
            best_acc = val_acc
            best_epoch = epoch
            state = {
                'best_acc': best_acc,
                'best_epoch': epoch,
                'best_net': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
                'optimizer': optimizer.state_dict(),
            }
            torch.save(state, os.path.join(args.log_path, 'model_weights.pth'))
            log.info('--- Best model saved at epoch %03d with acc %.4f ---' % (epoch, best_acc))

        log.info(
            'Epoch %03d: train_loss %.5f, test_loss %.5f, train_acc %.4f, test_acc %.4f, best_acc %.4f (epoch %03d)' % (
                epoch, train_loss, val_loss, train_acc, val_acc, best_acc, best_epoch))
        
        writer.add_scalars('Loss', {'val': val_loss, 'train': train_loss}, epoch + 1)
        writer.add_scalars('Acc', {'val': val_acc, 'train': train_acc}, epoch + 1)
        
    log.info('================================================================')
    log.info('Finish training! Best accuracy: {:.4f} at epoch {}.'.format(best_acc, best_epoch))
    log.info('Best weights saved to: {}'.format(os.path.join(args.log_path, 'model_weights.pth')))
    log.info('================================================================')
    
if __name__ == '__main__':
    from config.config import args
    init_config(args)
    main()
